# Guidance for Mavzuna and Cole

## Guidance about Task #1

Task #1 is to replicate the analyses in Crouch & Slade 2020. It begins with executing the code in this repository against the Kenya PRIMR and Kenya Tusome data and verifying the outcomes match what we reported in the paper. A few notes:

1. This code makes various assumptions about your folder structure and available user-installed packages.
    1. It assumes you have `data`, `bins`, and `logs` folders that are siblings to the _.do_-file you are executing
    1. It assumes that you have installed at least the packages detailed in the header comment of the `01_egra_preprocessing.do` file
1. The outputs are generally `.dta` files in the current folder and `.xlsx` folders in the `/bins` folder. (That is an inconsistency, not a considered/deliberate choice. It could be harmonized/rectified.)
1. The data visualization work (apart from the Lorenz curves) has largely been done in Excel. This is not optimal, and could be improved/rectified for the sake of consistency and reproducibility.
1. Execution of the _benchmark_- and _byschool_-level analyses is legitimately quite slow, even on a well-powered laptop. The default settings of the code as it stands currently attempt to strike a balance between verbosity and speed.
    1. You may wish to tweak them in your version of the code to better suit your needs.
    1. You may also wish to simply execute the higher-level analyses first so you have a spreadsheet to work with while these other analyses run in the background.

The code executes the `01_egra_preprocessing.do` file, which in turn depends upon the `00_apply_analysis.do` file. The latter simply defines the program that ingests the raw `.dta` files and generates the transformed/processed `.dta` output. `.xlsx` files are generated by the `01` file acting on the results of invoking the `00` file.

The `02_lorenz_exploration.do` depends upon `01` only to the extent that the necessary `.dta` files must be available. Otherwise it stands alone.

## Guidance about GitHub

The main purpose of GitHub is to enable easy version control. If you look at the various commit messages, you can see certain sections of code in red and other sections in green. This has nothing to do with code _quality_ or _utility_ per se. Red simply means that, for a given **version** of the codebase that was submitted (`commit`t-ed) to version control, those lines of code were removed or replaced. Green means that those lines of code were inserted (if they were not there before) or represent the new version of that line (if the line was previously present, but got modified).

GitHub works very well with simple, plaintext-based or plaintext-like files. (e.g., `.do`, `.md`, `.txt`, `.R`, `.Rmarkdown`, `.html`, etc.) It generally works very badly for many binary-type files (e.g., `.docx`, `.xlsx`) although it generally handles `.pdf`s reasonably well.

In order to make effective use of GitHub for version control on our analyses, our text, etc., please make use of Stata's [`dyndoc` library](https://www.stata.com/features/overview/markdown/). This provides a Stata wrapper for a [_literate programming_](https://en.wikipedia.org/wiki/Literate_programming)-based approach that will allow us to

1. keep our narrative/text under version control,
2. gracefully incorporate analytical code in a way that makes it easily auditable/reviewable, and
3. make use of GitHub's native commenting / threading / feedback features.

In brief, the approach involves

1. Writing your narrative in a `.txt` file
1. Embedding executable code blocks within that file using the paradigm shown below
```stata
~~~~            <- these mark the beginning of a code block
<<dd_do>>       <- this indicates the kind of execution/compilation Stata should do
summ orf if grade==2 & lat_round==3 /* This is the code /*
scalar pre1 = `r(mean)'             /* that will be both displayed and */
summ orf if grade==3 & lat_round==4 /* executed during the compilation */
scalar post1 = `r(mean)'            /* of your document. */
scalar loss1 = pre1 - post1
scalar pct_loss1 = loss1 / pre1
<</dd_do>>
~~~~
```
    + **N.B.** It is also possible to do in-line embedding of variables, graphs, tables, etc. This paradigm is extremely powerful: take some time to read up on it and review examples online. You may find reading up on the use of `R Markdown` documents in the R ecosystem to be useful - it is likely that documentation is more expansive than what you'll find on Stata given the prevalence of literate programming approaches in the data science community relative to the econ community. The principles are the same -- only the syntax is meaningfully different, and that is reasonably simple to pick up once you've caught the broader gist.
1. Calling `$ dyndoc the-file-to-be-compiled.txt` either interactively or from within a `.do` file to compile your output (e.g., an `.html` or `.pdf` or `.docx` file).
    1. For our purposes, please compile to HTML. You can then simply read it in your browser.

When you have analytical code that you are ready to have reviewed, you can upload it to GitHub. There is a whole command line-based process you can read up on if you'd like, but you can also simply do it via the GUI. Please do your work in a _branch_ of the codebase bearing your name so we don't have multiple versions of the `main` codebase running into each other. Please reach out for assistance if you require it.
